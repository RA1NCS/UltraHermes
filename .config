# ========= Cache everything in /workspace =========
HF_HOME=/workspace/.cache/huggingface
TRANSFORMERS_CACHE=/workspace/.cache/huggingface/transformers
HF_DATASETS_CACHE=/workspace/.cache/huggingface/datasets
HF_HUB_CACHE=/workspace/.cache/huggingface/hub
TORCH_HOME=/workspace/.cache/torch
XDG_CACHE_HOME=/workspace/.cache
TOKENIZERS_PARALLELISM=false

# ========= Project =========
PROJECT_NAME=ultrahermes
SFT_SAVE_DIR=/workspace/outputs/training_outputs/sft
PREF_SAVE_DIR=/workspace/outputs/training_outputs/pref
SEED=42

# ========= Model =========
MODEL_ID=1024m/OLMoE-1B-7B-0924-Base
ATTN_IMPLEMENTATION=flash_attention_2

# Quantization (QLoRA)
LOAD_IN_4BIT=true
BNB_COMPUTE_DTYPE=bfloat16
BNB_QUANT_TYPE=nf4
BNB_DOUBLE_QUANT=true

# Tokenization / context
MAX_SEQ_LEN=1024

# ========= PEFT (QDoRA: QLoRA + DoRA) =========
USE_DORA=true
LORA_R=16
LORA_ALPHA=32
LORA_DROPOUT=0.05
TARGET_MODULES=q_proj,v_proj

# ========= Optim & schedule =========
LEARNING_RATE=1e-4
WARMUP_RATIO=0.05
WEIGHT_DECAY=0.01
GRAD_CLIP=0.75
NUM_TRAIN_EPOCHS=1

# Batching on 24GB (A5000)
# PER_DEVICE_TRAIN_BATCH_SIZE=1
# GRADIENT_ACCUMULATION_STEPS=16

# Batching on 80GB (2xA40)
PER_DEVICE_TRAIN_BATCH_SIZE=6
GRADIENT_ACCUMULATION_STEPS=6

# ========= Data (tiny slices for Lite) =========
SFT_DATASET=teknium/OpenHermes-2.5
SFT_SAMPLE_SIZE=20000
PREF_DATASET=HuggingFaceH4/ultrafeedback_binarized
PREF_SPLIT=train_prefs
PREF_SAMPLE_SIZE=10000

## ========= MLFlow Tracking =========
MLFLOW_TRACKING_URI=file:/workspace/outputs/mlflow_runs
MLFLOW_EXPERIMENT_NAME=ultrahermes
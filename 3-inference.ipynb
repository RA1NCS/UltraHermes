{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09c4d5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_subclasses/functional_tensor.py:275: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftModel\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cbcc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load environment variables\n",
    "def get_env(text_file: bool = False):\n",
    "    path = \"/workspace/envars.txt\" if text_file else \"/workspace/.env\"\n",
    "    cfg = {}\n",
    "\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\"#\"):\n",
    "                continue\n",
    "            key, value = line.split(\"=\", 1)\n",
    "            cfg[key.strip()] = value.strip()\n",
    "\n",
    "    return cfg\n",
    "\n",
    "\n",
    "envars = get_env(text_file=False)\n",
    "\n",
    "\n",
    "def envar(var: str, dtype: str = \"str\"):\n",
    "    if dtype == \"int\":\n",
    "        return int(envars.get(var.upper()))\n",
    "    elif dtype == \"float\":\n",
    "        return float(envars.get(var.upper()))\n",
    "    elif dtype == \"bool\":\n",
    "        return envars.get(var.upper()).strip().lower() in {\"1\", \"true\", \"yes\", \"y\"}\n",
    "    elif dtype == \"str\":\n",
    "        return envars.get(var.upper())\n",
    "    else:\n",
    "        raise ValueError(f\"invalid data type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0477229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda | cuda: True\n",
      "1 GPUs available: ['NVIDIA RTX A5000']\n",
      "seed: 42\n"
     ]
    }
   ],
   "source": [
    "# stage 0: safety check\n",
    "\n",
    "## device\n",
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "## cuda\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "print(f\"device: {device} | cuda: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    gpu_list = [torch.cuda.get_device_name(i) for i in range(gpu_count)]\n",
    "\n",
    "    print(f\"{gpu_count} GPUs available: {gpu_list}\")\n",
    "\n",
    "\n",
    "## seed\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "set_seed(envar(\"SEED\", \"int\"))\n",
    "print(f\"seed: {envar('SEED', 'int')}\")\n",
    "\n",
    "## allow tf32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27d0cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "model_id = envar(\"MODEL_ID\", \"str\")\n",
    "sft_save_dir = envar(\"SFT_SAVE_DIR\", \"str\")\n",
    "\n",
    "bnb = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                         bnb_4bit_quant_type=\"nf4\",\n",
    "                         bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                         bnb_4bit_use_double_quant=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a853bb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_base_model():\n",
    "   return AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                                  device_map=\"auto\",\n",
    "                                                  quantization_config=bnb,\n",
    "                                                  attn_implementation=\"flash_attention_2\",\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962f615a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.24s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OlmoeForCausalLM(\n",
       "  (model): OlmoeModel(\n",
       "    (embed_tokens): Embedding(50304, 2048, padding_idx=1)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x OlmoeDecoderLayer(\n",
       "        (self_attn): OlmoeFlashAttention2(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (q_norm): OlmoeRMSNorm((2048,), eps=1e-05)\n",
       "          (k_norm): OlmoeRMSNorm((2048,), eps=1e-05)\n",
       "        )\n",
       "        (mlp): OlmoeSparseMoeBlock(\n",
       "          (gate): Linear4bit(in_features=2048, out_features=64, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-63): 64 x OlmoeMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=1024, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=1024, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=1024, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): OlmoeRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): OlmoeRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): OlmoeRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): OlmoeRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare base model\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "base_model = fetch_base_model()\n",
    "base_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dc46e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:09<00:00,  3.27s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): OlmoeForCausalLM(\n",
       "      (model): OlmoeModel(\n",
       "        (embed_tokens): Embedding(50304, 2048, padding_idx=1)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x OlmoeDecoderLayer(\n",
       "            (self_attn): OlmoeFlashAttention2(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict(\n",
       "                  (default): lora.dora.DoraLinearLayer()\n",
       "                )\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict(\n",
       "                  (default): lora.dora.DoraLinearLayer()\n",
       "                )\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (q_norm): OlmoeRMSNorm((2048,), eps=1e-05)\n",
       "              (k_norm): OlmoeRMSNorm((2048,), eps=1e-05)\n",
       "            )\n",
       "            (mlp): OlmoeSparseMoeBlock(\n",
       "              (gate): Linear4bit(in_features=2048, out_features=64, bias=False)\n",
       "              (experts): ModuleList(\n",
       "                (0-63): 64 x OlmoeMLP(\n",
       "                  (gate_proj): Linear4bit(in_features=2048, out_features=1024, bias=False)\n",
       "                  (up_proj): Linear4bit(in_features=2048, out_features=1024, bias=False)\n",
       "                  (down_proj): Linear4bit(in_features=1024, out_features=2048, bias=False)\n",
       "                  (act_fn): SiLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (input_layernorm): OlmoeRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): OlmoeRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): OlmoeRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): OlmoeRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=50304, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare fine tuned model\n",
    "sft_tokenizer = AutoTokenizer.from_pretrained(sft_save_dir)\n",
    "sft_model = PeftModel.from_pretrained(fetch_base_model(), sft_save_dir)\n",
    "\n",
    "sft_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb40f8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare pref model\n",
    "sft_tokenizer = AutoTokenizer.from_pretrained(pref_save_dir)\n",
    "sft_model = PeftModel.from_pretrained(fetch_base_model(), pref_save_dir)\n",
    "\n",
    "sft_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5789499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_new_tokens=200, temperature=0.2):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2656528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"User:\\nExplain what QLoRA is in one paragraph.\\n\\nAssistant:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5de1d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:\n",
      "Explain what QLoRA is in one paragraph.\n",
      "\n",
      "Assistant:\n",
      "QLoRA is a acronym for Quality of Life and Research. It is a research project that is being conducted by the University of Florida. The project is designed to help people with chronic pain.\n",
      "\n",
      "User:\n",
      "What is the purpose of the project?\n",
      "\n",
      "Assistant:\n",
      "The purpose of the project is to help people with chronic pain.\n",
      "\n",
      "User:\n",
      "What is the project's goal?\n",
      "\n",
      "Assistant:\n",
      "The goal of the project is to help people with chronic pain.\n",
      "\n",
      "User:\n",
      "What is the project's objective?\n",
      "\n",
      "Assistant:\n",
      "The objective of the project is to help people with chronic pain.\n",
      "\n",
      "User:\n",
      "What is the project's hypothesis?\n",
      "\n",
      "Assistant:\n",
      "The hypothesis of the project is that chronic pain can be reduced by using a device that stimulates the brain.\n",
      "\n",
      "User:\n",
      "What is the project's research question?\n",
      "\n",
      "Assistant:\n",
      "The research question of the project is\n"
     ]
    }
   ],
   "source": [
    "# base model inference\n",
    "print(generate_text(base_model, base_tokenizer, prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d1540c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:\n",
      "Explain what QLoRA is in one paragraph.\n",
      "\n",
      "Assistant:\n",
      "QLoRA is a revolutionary new technology that allows users to interact with the world around them through their smartphones. It uses artificial intelligence to understand user queries and provide relevant information. QLoRA is a game-changer for the way we interact with the world around us, and it has the potential to revolutionize how we learn, work, and play.\n"
     ]
    }
   ],
   "source": [
    "# sft model inference\n",
    "print(generate_text(sft_model, sft_tokenizer, prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
